{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling for Gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames\n",
    "filenames = []\n",
    "for file in glob.glob(\"../Dataset/rawData1/*.txt\"):\n",
    "    filenames.append(file.split('\\\\')[-1].split('.')[0])\n",
    "\n",
    "# Get sectors\n",
    "masterfilepath = '../Dataset/SectorTickerData.xlsx'\n",
    "data = pd.read_excel(masterfilepath)    \n",
    "data['GSECTOR'].replace(np.nan,0, inplace=True)\n",
    "\n",
    "# Get (ticker,sector) set\n",
    "temp = set()\n",
    "for i in range(len(data)):\n",
    "    temp.add((data['Ticker'][i],data['GSECTOR'][i]))\n",
    "set_ticker_sector = pd.DataFrame(temp,columns=['Ticker','Sector'])\n",
    "\n",
    "# Curate for further use\n",
    "sectors = list(pd.unique(data['GSECTOR']))\n",
    "years = list(pd.unique([data['Date'][i]%10000 for i in range(len(data))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4Jj-7F-v5rdJ"
   },
   "outputs": [],
   "source": [
    "# Generate 3D array with filenames for given year(2017-20) in given sector\n",
    "ticker_in_sectors = [[[] for _ in range(len(years)-2)] for _ in range(len(sectors))]\n",
    "for i in filenames:\n",
    "    tic = i.split('_')[0]\n",
    "    year = int(i.split('_')[1])%10000\n",
    "    if year != 2015 and year!=2016:\n",
    "        idx_x = sectors.index(set_ticker_sector['Sector'][set_ticker_sector['Ticker'].tolist().index(tic)])\n",
    "        idx_y = years.index(year)\n",
    "        ticker_in_sectors[idx_x][idx_y].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tVykKBzlEenq"
   },
   "outputs": [],
   "source": [
    "# Choose two ticker per year from each sector\n",
    "def get_gold_label_tickers(seed_used, tickers):\n",
    "  random.seed(seed_used)\n",
    "  tickers_to_be_used = []\n",
    "  tickers_data = []\n",
    "  for i in range(len(tickers)):\n",
    "        for j in range(len(tickers[i])):\n",
    "              if len(tickers[i][j])>2:\n",
    "                idx = random.sample(tickers[i][j], 2)\n",
    "                tickers_to_be_used.append(idx)\n",
    "                for k in range(len(idx)):\n",
    "                  tickers_data.append([sectors[i],idx[k].split('_')[0],years[j],idx[k]])\n",
    "  pd.DataFrame(tickers_data,columns=['Sector','Ticker','Year','Filename']).to_excel('../Dataset/GoldLabels_TickerSectorMapping.xlsx')\n",
    "  return tickers_to_be_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tickers = get_gold_label_tickers(1729,ticker_in_sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gold label data in different folder\n",
    "for file_list in gold_tickers:\n",
    "    for idx in range(len(file_list)):\n",
    "        f = open('../Dataset/rawData1/'+file_list[idx]+'.txt',\"r\")\n",
    "        copy = open('../Dataset/rawDataGoldLabel/'+file_list[idx]+'.txt',\"wt\")\n",
    "        line = f.read()\n",
    "        copy.write(str(line))\n",
    "        f.close()\n",
    "        copy.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data for BERT-G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "label = []\n",
    "\n",
    "for file in glob.glob(\"../Dataset/GoldLabelledData/*.xlsx\"):\n",
    "  data = pd.read_excel(file)\n",
    "  for i in range(len(data)):\n",
    "    sentences.append(data['sentence'][i])\n",
    "    label.append(data['Manual'][i])\n",
    "df = pd.DataFrame(np.transpose([label,sentences]),columns=['label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take train-test-validation = 80-10-10 \n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.10, random_state=1729)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df['text'], df['label'], test_size=0.10, random_state=13832)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(df['text'], df['label'], test_size=0.10, random_state=110656)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(df['text'], df['label'], test_size=0.10, random_state=42)\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(df['text'], df['label'], test_size=0.10, random_state=149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice validation data \n",
    "X_valid = X_train[:len(X_test)]\n",
    "X_valid2 = X_train2[:len(X_test2)]\n",
    "X_valid3 = X_train3[:len(X_test3)]\n",
    "X_valid4 = X_train4[:len(X_test4)]\n",
    "X_valid5 = X_train5[:len(X_test5)]\n",
    "\n",
    "y_valid = y_train[:len(y_test)]\n",
    "y_valid2 = y_train2[:len(y_test2)]\n",
    "y_valid3 = y_train3[:len(y_test3)]\n",
    "y_valid4 = y_train4[:len(y_test4)]\n",
    "y_valid5 = y_train5[:len(y_test5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe \n",
    "train1 = pd.DataFrame(np.transpose([y_train[len(X_valid):],X_train[len(X_valid):]]),columns=['label','text'])\n",
    "train2 = pd.DataFrame(np.transpose([y_train2[len(X_valid2):],X_train2[len(X_valid2):]]),columns=['label','text'])\n",
    "train3 = pd.DataFrame(np.transpose([y_train3[len(X_valid3):],X_train3[len(X_valid3):]]),columns=['label','text'])\n",
    "train4 = pd.DataFrame(np.transpose([y_train4[len(X_valid4):],X_train4[len(X_valid4):]]),columns=['label','text'])\n",
    "train5 = pd.DataFrame(np.transpose([y_train5[len(X_valid5):],X_train5[len(X_valid5):]]),columns=['label','text'])\n",
    "\n",
    "valid1 = pd.DataFrame(np.transpose([y_valid,X_valid]),columns=['label','text'])\n",
    "valid2 = pd.DataFrame(np.transpose([y_valid2,X_valid2]),columns=['label','text'])\n",
    "valid3 = pd.DataFrame(np.transpose([y_valid3,X_valid3]),columns=['label','text'])\n",
    "valid4 = pd.DataFrame(np.transpose([y_valid4,X_valid4]),columns=['label','text'])\n",
    "valid5 = pd.DataFrame(np.transpose([y_valid5,X_valid5]),columns=['label','text'])\n",
    "\n",
    "test1 = pd.DataFrame(np.transpose([y_test,X_test]),columns=['label','text'])\n",
    "test2 = pd.DataFrame(np.transpose([y_test2,X_test2]),columns=['label','text'])\n",
    "test3 = pd.DataFrame(np.transpose([y_test3,X_test3]),columns=['label','text'])\n",
    "test4 = pd.DataFrame(np.transpose([y_test4,X_test4]),columns=['label','text'])\n",
    "test5 = pd.DataFrame(np.transpose([y_test5,X_test5]),columns=['label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the files\n",
    "train1.to_csv('../Dataset/BERT-G/train1.csv', index=False)\n",
    "valid1.to_csv('../Dataset/BERT-G/valid1.csv', index=False)\n",
    "test1.to_csv('../Dataset/BERT-G/test1.csv', index=False)\n",
    "\n",
    "train2.to_csv('../Dataset/BERT-G/train2.csv', index=False)\n",
    "valid2.to_csv('../Dataset/BERT-G/valid2.csv', index=False)\n",
    "test2.to_csv('../Dataset/BERT-G/test2.csv', index=False)\n",
    "\n",
    "train3.to_csv('../Dataset/BERT-G/train3.csv', index=False)\n",
    "valid3.to_csv('../Dataset/BERT-G/valid3.csv', index=False)\n",
    "test3.to_csv('../Dataset/BERT-G/test3.csv', index=False)\n",
    "\n",
    "train4.to_csv('../Dataset/BERT-G/train4.csv', index=False)\n",
    "valid4.to_csv('../Dataset/BERT-G/valid4.csv', index=False)\n",
    "test4.to_csv('../Dataset/BERT-G/test4.csv', index=False)\n",
    "\n",
    "train5.to_csv('../Dataset/BERT-G/train5.csv', index=False)\n",
    "valid5.to_csv('../Dataset/BERT-G/valid5.csv', index=False)\n",
    "test5.to_csv('../Dataset/BERT-G/test5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data for BERT-W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3YQdgibfPN3"
   },
   "outputs": [],
   "source": [
    "# To ensure Gold Labelled files are not included in BERT training data\n",
    "filenames = []\n",
    "for file in glob.glob(\"../Dataset/GoldLabelledData/*.xlsx\"):\n",
    "  filenames.append(file.split('\\\\')[1].split('.')[0])\n",
    "\n",
    "# For further use\n",
    "data = pd.read_excel('../Dataset/MasterFile.xlsx')\n",
    "data['Sector'].replace(np.nan,0, inplace=True)\n",
    "sectors = list(pd.unique(data['Sector']))\n",
    "years = list(pd.unique([data['Date'][i]%10000 for i in range(len(data))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Jj-7F-v5rdJ"
   },
   "outputs": [],
   "source": [
    "# Find tickers that belong to a given sector excluding the GoldLabelled Data\n",
    "tickers_in_sector = [[] for _ in range(len(sectors))]\n",
    "for i in range(len(data)):\n",
    "    idx = sectors.index(data['Sector'][i])\n",
    "    if data['Filename'][i] not in filenames:\n",
    "      tickers_in_sector[idx].append(data['Ticker'][i])\n",
    "\n",
    "# Find unique tickers\n",
    "for i in range(len(tickers_in_sector)):\n",
    "  tickers_in_sector[i] = pd.unique(tickers_in_sector[i])\n",
    "count_of_ticker = [len(ticker_in_sectors[i]) for i in range(len(ticker_in_sectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVykKBzlEenq"
   },
   "outputs": [],
   "source": [
    "# Choose n% of random tickers from each sector depending on seed_used\n",
    "def get_weak_label_tickers(seed_used, n, tickers):\n",
    "  random.seed(seed_used)\n",
    "  tickers_to_be_used = []\n",
    "  for i in range(len(tickers)):\n",
    "    idx = random.sample(tickers[i].tolist(), int(n*len(tickers[i])))\n",
    "    tickers_to_be_used.append(idx)\n",
    "  return tickers_to_be_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylLRe6E-F_sy"
   },
   "outputs": [],
   "source": [
    "# Take 50% of ticker from each sector\n",
    "n = 0.5\n",
    "tickers_to_be_used1 = get_weak_label_tickers(1729,n,ticker_in_sectors)\n",
    "tickers_to_be_used2 = get_weak_label_tickers(13832,n,ticker_in_sectors)\n",
    "tickers_to_be_used3 = get_weak_label_tickers(110656,n,ticker_in_sectors)\n",
    "tickers_to_be_used4 = get_weak_label_tickers(42,n,ticker_in_sectors)\n",
    "tickers_to_be_used5 = get_weak_label_tickers(149,n,ticker_in_sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyg70ACgbP99"
   },
   "outputs": [],
   "source": [
    "# Get equal number of inclaim and outclaim sentences from the files obtained using previous step\n",
    "def get_training_data(tickers_to_be_used,save_file_as,files_used_list_saved_as):\n",
    "  count=0\n",
    "\n",
    "  # Select unique tickers to form training data\n",
    "  ticker_set = set()\n",
    "  for i in range(len(tickers_to_be_used)):\n",
    "    for j in range(len(tickers_to_be_used[i])):\n",
    "      ticker_set.add(tickers_to_be_used[i][j])\n",
    "\n",
    "  sentences = []\n",
    "  inclaim = []\n",
    "  total_count=0\n",
    "  local_count=[]\n",
    "  files_used = []\n",
    "  completed_ticker = set()\n",
    "\n",
    "  # Get training data for given seed\n",
    "  for file in glob.glob(\"../Dataset/WeakLabelledData/*.xlsx\"):\n",
    "      \n",
    "      fileticker, fileyear = file.split('.')[-2].split('\\\\')[-1].split('_') \n",
    "      filename = file.split('\\\\')[-1]\n",
    "      fileyear = int(fileyear)%10000\n",
    "\n",
    "      # Ensure that tickers are not repeated and they belong to only the required set\n",
    "      if (fileticker,fileyear) not in completed_ticker and fileticker in ticker_set:\n",
    "        data2 = pd.read_excel(file)\n",
    "        inclaim_num = np.count_nonzero(data2['Inclaim'])\n",
    "        outclaim_num = len(data2) - inclaim_num \n",
    "        \n",
    "        # To ensure equal number of inclaim and out of claim sentences\n",
    "        count = min(inclaim_num,outclaim_num)\n",
    "        inclaim_num = count\n",
    "        outclaim_num = count\n",
    "\n",
    "        for i in range(len(data2)):\n",
    "          if data2['Inclaim'][i]>0 and inclaim_num>0:\n",
    "            inclaim.append(1)\n",
    "            sentences.append(data2['sentence'][i])\n",
    "            inclaim_num-=1\n",
    "            files_used.append(filename)\n",
    "          elif outclaim_num>0:\n",
    "            inclaim.append(0)\n",
    "            sentences.append(data2['sentence'][i])\n",
    "            outclaim_num-=1\n",
    "            files_used.append(filename)\n",
    "          elif inclaim_num==0 and outclaim_num==0:\n",
    "            break\n",
    "        local_count.append(2*count)\n",
    "        total_count+=2*count\n",
    "        completed_ticker.add((fileticker,fileyear))\n",
    "\n",
    "  # Save training data in new file\n",
    "  df = pd.DataFrame(np.transpose([inclaim,sentences]),columns=['label','text'])\n",
    "  df.to_csv(save_file_as,index=False)\n",
    "  pd.DataFrame(files_used,columns=['Filename']).to_csv(files_used_list_saved_as,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_training_data(tickers_to_be_used1,'../Dataset/BERT-W/train1.csv','../Dataset/BERT-W/list1.csv')\n",
    "get_training_data(tickers_to_be_used2,'../Dataset/BERT-W/train2.csv','../Dataset/BERT-W/list2.csv')\n",
    "get_training_data(tickers_to_be_used3,'../Dataset/BERT-W/train3.csv','../Dataset/BERT-W/list3.csv')\n",
    "get_training_data(tickers_to_be_used4,'../Dataset/BERT-W/train4.csv','../Dataset/BERT-W/list4.csv')\n",
    "get_training_data(tickers_to_be_used5,'../Dataset/BERT-W/train5.csv','../Dataset/BERT-W/list5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data BERT-WG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data of BERT-G\n",
    "tg1 = pd.read_csv('../Dataset/BERT-G/train1.csv')\n",
    "tg2 = pd.read_csv('../Dataset/BERT-G/train2.csv')\n",
    "tg3 = pd.read_csv('../Dataset/BERT-G/train3.csv')\n",
    "tg4 = pd.read_csv('../Dataset/BERT-G/train4.csv')\n",
    "tg5 = pd.read_csv('../Dataset/BERT-G/train5.csv')\n",
    "\n",
    "# Read training data of BERT-W\n",
    "tw1 = pd.read_csv('../Dataset/BERT-W/train1.csv')\n",
    "tw2 = pd.read_csv('../Dataset/BERT-W/train2.csv')\n",
    "tw3 = pd.read_csv('../Dataset/BERT-W/train3.csv')\n",
    "tw4 = pd.read_csv('../Dataset/BERT-W/train4.csv')\n",
    "tw5 = pd.read_csv('../Dataset/BERT-W/train5.csv')\n",
    "\n",
    "# Concatenate training data of BERT-G and BERT-W to form training data of BERT-WG\n",
    "twg1 = pd.concat([tw1, tg1], ignore_index=True)\n",
    "twg2 = pd.concat([tw2, tg2], ignore_index=True)\n",
    "twg3 = pd.concat([tw3, tg3], ignore_index=True)\n",
    "twg4 = pd.concat([tw4, tg4], ignore_index=True)\n",
    "twg5 = pd.concat([tw5, tg5], ignore_index=True)\n",
    "\n",
    "# Save the files\n",
    "twg1.to_csv('../Dataset/BERT-WG/train1.csv',index=False)\n",
    "twg2.to_csv('../Dataset/BERT-WG/train2.csv',index=False)\n",
    "twg3.to_csv('../Dataset/BERT-WG/train3.csv',index=False)\n",
    "twg4.to_csv('../Dataset/BERT-WG/train4.csv',index=False)\n",
    "twg5.to_csv('../Dataset/BERT-WG/train5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf2350cba43e1d06d9c1f803cf97a1d209fb9586cdf73481a8f23553c7381136"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
